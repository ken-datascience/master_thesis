{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt #描画ライブラリ\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import pathlib\n",
    "import glob\n",
    "import math\n",
    "import codecs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total demand & Generation by different type of plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the common path where dataset put\n",
    "path ='/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/actual_generation_demand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tohoku (base)\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Tohoku/juyo_*.csv\")) \n",
    "frame = pd.DataFrame()\n",
    "col_names = [\"DateTime\", \"TotalDemand_TOH\", \"Water_TOH\", \"Thermal_TOH\", \"Nuclear_TOH\", \"PV_TOH\", \n",
    "             \"PVCurtailment_TOH\", \"Wind_TOH\", \"WindCurtailment_TOH\", \"Geothermal_TOH\", \"Biomass_TOH\", \n",
    "             \"PumpedStorage_TOH\", \"Interconnection_TOH\"]\n",
    "list_ = []\n",
    "for file_path in allFiles:\n",
    "    # It has original columns. Do not set new name of columns\n",
    "    df = pd.read_csv(file_path, sep=',', header=0, delimiter=\",\", names=col_names, encoding='shift_jis') \n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "\n",
    "# Base index\n",
    "DateTime_Tohoku = frame[\"DateTime\"]\n",
    "\n",
    "frame.to_csv(path + \"/Demand_plant_tohoku.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hokkaido\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Hokkaido/sup_dem_results_*.csv\")) \n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_HOK\", \"Nuclear_HOK\", \"Thermal_HOK\", \"Water_HOK\", \"Geothermal_HOK\", \"Biomass_HOK\", \"PV_HOK\", \n",
    "             \"PVCurtailment_HOK\", \"Wind_HOK\", \"WindCurtailment_HOK\", \"PumpedStorage_HOK\", \"Interconnection_HOK\", \"TotalSupply_HOK\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"shift_jis\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, delimiter=\",\", names=col_names, skiprows=[0,1,2,3]) \n",
    "#         df = df.dropna(how='all', axis=0)\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner')\n",
    "frame = frame.dropna(how='all', axis=0).reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "frame = frame.drop([\"Date\", \"Time\"], axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/Demand_plant_Hokkaido.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokyo\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Tokyo/area-*.csv\"))\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_TKO\", \"Nuclear_TKO\", \"Thermal_TKO\", \"Water_TKO\", \"Geothermal_TKO\", \"Biomass_TKO\", \"PV_TKO\",\n",
    "             \"PVCurtailment_TKO\", \"Wind_TKO\", \"WindCurtailment_TKO\", \"PumpedStorage_TKO\", \"Interconnection_TKO\", \"TotalSupply_TKO\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, delimiter=\",\", names=col_names, skiprows=[0,1,2])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[0:41664]\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "frame = frame.drop([\"Date\", \"Time\"], axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/Demand_plant_Tokyo.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chubu\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Chubu/areabalance_current_term_*.csv\")) \n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_CHB\", \"Nuclear_CHB\", \"Thermal_CHB\", \"Water_CHB\", \"Geothermal_CHB\", \"Biomass_CHB\", \"PV_CHB\", \n",
    "             \"PVCurtailment_CHB\", \"Wind_CHB\", \"WindCurtailment_CHB\",\"PumpedStorage_CHB\", \"Interconnection_CHB\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0,1,2,3])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[0:41664]\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "frame = frame.drop([\"Date\", \"Time\"], axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/Demand_plant_Chubu.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hokuriku\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Hokuriku/area_jisseki_rikuden*.csv\"))\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_HKU\", \"Nuclear_HKU\", \"Thermal_HKU\", \"Water_HKU\", \"Geothermal_HKU\", \"Biomass_HKU\", \"PV_HKU\", \n",
    "             \"PVCurtailment_HKU\", \"Wind_HKU\", \"WindCurtailment_HKU\",\"PumpedStorage_HKU\", \"Interconnection_HKU\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0,1,2,3,4], usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner')\n",
    "frame = frame.dropna(how='all', axis=0)\n",
    "# delete row contains \"TIME\" in TIme column\n",
    "frame = frame[~frame['Time'].str.contains('TIME')].reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[0:41664]\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "frame = frame.drop([\"Date\", \"Time\"], axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/Demand_plant_Hokuriku.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kansai\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Kansai/area_jyukyu_jisseki_*.csv\")) \n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"DateTime\", \"TotalDemand_KAN\", \"Nuclear_KAN\", \"Thermal_KAN\", \"Water_KAN\", \"Geothermal_KAN\", \"Biomass_KAN\", \"PV_KAN\", \n",
    "             \"PVCurtailment_KAN\", \"Wind_KAN\", \"WindCurtailment_KAN\",\"PumpedStorage_KAN\", \"Interconnection_KAN\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner')\n",
    "frame = frame.dropna(how='all', axis=0).reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[0:41664]\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "frame.to_csv(path + \"/Demand_plant_Kansai.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chugoku\n",
    "file_path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/actual_generation_demand/Chugoku/eria_jyukyu.csv\"\n",
    "\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_CHG\", \"Nuclear_CHG\", \"Thermal_CHG\", \"Water_CHG\", \"Geothermal_CHG\", \"Biomass_CHG\", \"PV_CHG\", \n",
    "             \"PVCurtailment_CHG\", \"Wind_CHG\", \"WindCurtailment_CHG\",\"PumpedStorage_CHG\", \"Interconnection_CHG\"]\n",
    "\n",
    "# Avoiding UnicodeDecodeError\n",
    "with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "    # Read csv setting col_names\n",
    "    df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0,1])\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index()\n",
    "df = df.drop(\"index\", axis=1)\n",
    "# Pick up until 12/31 23:00\n",
    "df = df[:25560]\n",
    "\n",
    "# Adjust DateTime column from 2018/2/1 0:00 to 12/31 23:00\n",
    "df[\"DateTime\"] = DateTime_Tohoku[16104:].reset_index().drop(\"index\", axis=1)\n",
    "df = df.drop([\"Date\", \"Time\"], axis=1)\n",
    "\n",
    "# Save csv\n",
    "df.to_csv(path + \"/Demand_plant_Chugoku.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shikoku\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Shikoku/jukyu*.csv\")) # 指定したフォルダーの全CSVファイルを変数に代入します\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_SHI\", \"Nuclear_SHI\", \"Thermal_SHI\", \"Water_SHI\", \"Biomass_SHI\",  \"PV_SHI\", \n",
    "             \"PVCurtailment_SHI\", \"Wind_SHI\", \"WindCurtailment_SHI\",\"PumpedStorage_SHI\", \"Interconnection_SHI\", \"TotalSupply_SHI\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv skipping the last row which is blank\n",
    "        df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0,1,2,3,4,5,6,7], usecols=[0,1,2,3,4,5,7,8,9,10,11,12,13,14])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner')\n",
    "frame = frame.dropna(how='all', axis=0).reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[0:41664]\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "frame = frame.drop([\"Date\", \"Time\"], axis=1)\n",
    "frame.to_csv(path + \"/Demand_plant_Shikoku.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kyushu\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Kyushu/area_jyukyu_jisseki_*.csv\")) \n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"DateTime\", \"TotalDemand_KYU\", \"Nuclear_KYU\", \"Thermal_KYU\", \"Water_KYU\", \"Geothermal_KYU\", \"Biomass_KYU\", \"PV_KYU\", \n",
    "             \"PVCurtailment_KYU\", \"Wind_KYU\", \"WindCurtailment_KYU\",\"PumpedStorage_KYU\", \"Interconnection_KYU\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner')\n",
    "frame = frame.dropna(how='all', axis=0).reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "\n",
    "frame.to_csv(path + \"/Demand_plant_Kyushu.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okinawa\n",
    "# Search file path on the folder using wildcard\n",
    "allFiles = sorted(glob.glob(path + \"/Okinawa/*.csv\")) \n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"TotalDemand_OKI\", \"Thermal_OKI\", \"Water_OKI\", \"Biomass_OKI\", \"PV_OKI\", \"PVCurtailment_OKI\", \n",
    "             \"Wind_OKI\", \"WindCurtailment_OKI\",\"TotalSupply_OKI\"]\n",
    "# Read the files\n",
    "for file_path in allFiles:\n",
    "    # Avoiding UnicodeDecodeError\n",
    "    with codecs.open(file_path, \"r\", \"Shift-JIS\", \"ignore\") as file_:\n",
    "        # Read csv\n",
    "        df = pd.read_csv(file_, header=0, delimiter=\",\", names=col_names, skiprows=[0,1,2,3,4,5], usecols=[0,1,2,4,5,6,7,8,9,10,11])\n",
    "        list_.append(df)\n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[0:41664]\n",
    "\n",
    "# Adjust DateTime column\n",
    "frame[\"DateTime\"] = DateTime_Tohoku\n",
    "# frame = frame.drop([\"Date\", \"Time\"], axis=1)\n",
    "frame.to_csv(path + \"/Demand_plant_Okinawa.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit_actual_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/actual_generation_demand/Okinawa/2020.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[]\n",
    "frame = pd.DataFrame()\n",
    "area = \"Hokkaido\"\n",
    "path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/Fit_actual_prediction/\"\n",
    "\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"ForecastPV_\"+area, \"ForecastWind_\"+area]\n",
    "# for file_path in allFiles:\n",
    "allFiles = sorted(glob.glob(path + area + \"/expect_*.csv\")) \n",
    "\n",
    "for file_path in allFiles:\n",
    "    # Read csv\n",
    "    df = pd.read_csv(file_path, header=0, delimiter=\",\", names=col_names, skiprows=[0, 1], usecols=[0,1,3,4])\n",
    "    list_.append(df)\n",
    "    \n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/FITforecast_\" + area + \".csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[]\n",
    "frame = pd.DataFrame()\n",
    "area = \"Tohoku\"\n",
    "path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/Fit_actual_prediction/\"\n",
    "\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"ForecastPV_\"+area, \"ForecastWind_\"+area]\n",
    "# for file_path in allFiles:\n",
    "allFiles = sorted(glob.glob(path + area + \"/*.csv\")) \n",
    "\n",
    "for file_path in allFiles:\n",
    "    # Read csv\n",
    "    df = pd.read_csv(file_path, header=0, delimiter=\",\", names=col_names, skiprows=[0,1], usecols=[0,1,4,6])\n",
    "    list_.append(df)\n",
    "    \n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "frame = frame[frame.index < 83328]\n",
    "frame = frame.fillna(0)\n",
    "\n",
    "frame.to_csv(path + \"/FITforecast_\" + area + \".csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83323   2020-12-31 21:30:00\n",
       "83324   2020-12-31 22:00:00\n",
       "83325   2020-12-31 22:30:00\n",
       "83326   2020-12-31 23:00:00\n",
       "83327   2020-12-31 23:30:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateTime = frame[\"DateTime\"] = pd.to_datetime(frame[\"Date\"].astype(str) + \" \" + frame[\"Time\"].astype(str), format='%Y/%m/%d %H:%M')\n",
    "DateTime.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[]\n",
    "frame = pd.DataFrame()\n",
    "area = \"Tokyo\"\n",
    "path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/Fit_actual_prediction/\"\n",
    "\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"ForecastPV_\"+area, \"ForecastWind_\"+area]\n",
    "# for file_path in allFiles:\n",
    "allFiles = sorted(glob.glob(path + area + \"/Tokyo_fit-*.csv\")) \n",
    "\n",
    "for file_path in allFiles:\n",
    "    # Read csv\n",
    "    df = pd.read_csv(file_path, header=0, delimiter=\",\", names=col_names, skiprows=[0], usecols=[0,1,3,5])\n",
    "    list_.append(df)\n",
    "    \n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/FITforecast_\" + area + \".csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    00:00\n",
       "1    00:30\n",
       "2    01:00\n",
       "3    01:30\n",
       "4    02:00\n",
       "dtype: object"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Half hourly table\n",
    "HH_time = pd.date_range('2020/01/01', periods=48,  freq='30min').strftime('%H:%M')\n",
    "HH_time = pd.Series(HH_time.str[-8:])\n",
    "HH_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[]\n",
    "area = \"Chubu\"\n",
    "path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/Fit_actual_prediction/\"\n",
    "# for file_path in allFiles:\n",
    "allFiles = sorted(glob.glob(path + area + '/tokureihatsuden_*.xls')) \n",
    "\n",
    "# Read the sheat no.0 and 2 which are for the forecast information\n",
    "sheet_num = [0, 2]\n",
    "\n",
    "for file in allFiles:\n",
    "    for i in sheet_num:\n",
    "        df_sheet = pd.read_excel(file, sheet_name=i, skiprows=[0,1,2, 52], index_col=0)\n",
    "        # Add \"Time\" Column\n",
    "        df_sheet = df_sheet.reset_index(drop=True)\n",
    "        df_sheet[\"Time\"] = HH_time\n",
    "        df_sheet = df_sheet.set_index(\"Time\")\n",
    "       # Make the dataframe for PV and Wind separately, and organize the format\n",
    "        if i == 0:\n",
    "            df_sheet_0 = df_sheet.T\n",
    "            df_sheet_0 = pd.DataFrame(df_sheet_0.stack()).reset_index()\n",
    "            df_sheet_0 = df_sheet_0.rename(columns={'level_0': 'Date', 0:str(i)})\n",
    "        else:\n",
    "            df_sheet_2 = df_sheet.T\n",
    "            df_sheet_2 = pd.DataFrame(df_sheet_2.stack()).reset_index()\n",
    "            df_sheet_2 = df_sheet_2.rename(columns={'level_0': 'Date', 0:str(i)})        \n",
    "    # Concat all the sheet to make a file\n",
    "    df_file = pd.merge(df_sheet_0, df_sheet_2, how='left', on=['Date', 'Time'])\n",
    "    df_file = df_file.rename(columns={'0': 'Forecast_PV_Chubu', '2':'Forecast_Wind_Chubu'})\n",
    "    file_list.append(df_file)\n",
    "    \n",
    "# Merge all the csv file on the folder into a file\n",
    "frame = pd.concat(file_list, join='inner').reset_index()\n",
    "frame[\"DateTime\"] = DateTime\n",
    "frame[\"Date\"] = pd.to_datetime(DateTime).dt.date\n",
    "frame[\"Time\"] = pd.to_datetime(DateTime).dt.time\n",
    "frame = frame.drop([\"index\", \"DateTime\"], axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/FITforecast_\" + area + \".csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Forecast_PV_Chubu</th>\n",
       "      <th>Forecast_Wind_Chubu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83323</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83324</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83325</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>22:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83326</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83327</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date      Time  Forecast_PV_Chubu Forecast_Wind_Chubu\n",
       "83323  2020-12-31  21:30:00                0.0              104489\n",
       "83324  2020-12-31  22:00:00                0.0              105380\n",
       "83325  2020-12-31  22:30:00                0.0              106476\n",
       "83326  2020-12-31  23:00:00                0.0              107466\n",
       "83327  2020-12-31  23:30:00                0.0              108352"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[]\n",
    "frame = pd.DataFrame()\n",
    "area = \"Hokuriku\"\n",
    "path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/Fit_actual_prediction/\"\n",
    "\n",
    "# Set the original common columns\n",
    "col_names = [\"Date\", \"Time\", \"ForecastPV_\"+area, \"ForecastWind_\"+area]\n",
    "# for file_path in allFiles:\n",
    "allFiles = sorted(glob.glob(path + area + \"/fit1_rikuden*.csv\")) \n",
    "\n",
    "for file_path in allFiles:\n",
    "    # Read csv\n",
    "    df = pd.read_csv(file_path, header=0, delimiter=\",\", names=col_names, skiprows=[0], usecols=[0,1,3,5])\n",
    "    list_.append(df)\n",
    "    \n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "frame = frame.drop(\"index\", axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/FITforecast_\" + area + \".csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[]\n",
    "frame = pd.DataFrame()\n",
    "area = \"Kansai\"\n",
    "path = \"/Users/kenotsu/Documents/master_thesis/Datasets/Master_thesis/Fit_actual_prediction/\"\n",
    "\n",
    "# Set the original common columns\n",
    "col_names = [\"DateTime\", \"ForecastPV_\"+area, \"ForecastWind_\"+area]\n",
    "# for file_path in allFiles:\n",
    "allFiles = sorted(glob.glob(path + area + \"/fit1_soutei_jisseki_*.csv\")) \n",
    "\n",
    "for file_path in allFiles:\n",
    "    # Read csv\n",
    "    df = pd.read_csv(file_path, header=0, delimiter=\",\", names=col_names, skiprows=[0], usecols=[0,1,3])\n",
    "    list_.append(df)\n",
    "    \n",
    "# Concat all the csv file on the folder\n",
    "frame = pd.concat(list_, join='inner').reset_index()\n",
    "# frame[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"], format='%Y/%m/%d %H:%M')\n",
    "frame[\"Date\"] = pd.to_datetime(df[\"DateTime\"]).dt.date\n",
    "# frame[\"Time\"] = pd.to_datetime(df[\"DateTime\"]).dt.time\n",
    "frame = frame.drop([\"index\"], axis=1)\n",
    "\n",
    "frame.to_csv(path + \"/FITforecast_\" + area + \".csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018/12/12 12:00    4\n",
       "2018/12/12 12:30    4\n",
       "2018/12/14 12:30    2\n",
       "2018/12/12 3:00     2\n",
       "2018/12/12 8:00     2\n",
       "                   ..\n",
       "2016/9/13 19:00     1\n",
       "2017/6/21 22:30     1\n",
       "2020/6/1 17:30      1\n",
       "2019/3/10 23:30     1\n",
       "2018/11/10 17:00    1\n",
       "Name: DateTime, Length: 83215, dtype: int64"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[\"DateTime\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marge the data of Actual_generation(Tohoku_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Actual generation_dataset from 2016-08-01\n",
    "# path ='/Users/kenotsu/Documents/Datasets/Master_thesis/Actual_generation_5min/Actual_generation_5min_until_2020-4-14'\n",
    "# allFiles = sorted(glob.glob(path + \"/*.csv\")) # 指定したフォルダーの全CSVファイルを変数に代入します\n",
    "# frame = pd.DataFrame()\n",
    "# list_ = []\n",
    "# col_names = [\"DATE\", \"TIME\", \"当日実績(5分間隔値)(万kW)\"]\n",
    "# for file_ in allFiles:\n",
    "#     df = pd.read_csv(file_, sep=',', header=36, encoding='shift_jis', names=col_names) # csvをデータフレームとして読み込む\n",
    "#     list_.append(df)\n",
    "# frame = pd.concat(list_, join='inner') # joinをinnerに指定\n",
    "# frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Actual_generation_5min' + \"/Actual_generation1.csv\", encoding=\"shift_jis\", index=False)\n",
    "\n",
    "\n",
    "# #Actual generation_dataset from 2020-04-14\n",
    "# path ='/Users/kenotsu/Documents/Datasets/Master_thesis/Actual_generation_5min/Actual_generation_5min_from_2020-4-15'\n",
    "# allFiles = sorted(glob.glob(path + \"/*.csv\")) # 指定したフォルダーの全CSVファイルを変数に代入します\n",
    "# frame = pd.DataFrame()\n",
    "# list_ = []\n",
    "# col_names = [\"DATE\", \"TIME\", \"当日実績(5分間隔値)(万kW)\", \"太陽光発電実績(5分間隔値)(万kW)\", \"風力発電実績(5分間隔値)(万kW)\"]\n",
    "# for file_ in allFiles:\n",
    "#     df = pd.read_csv(file_, sep=',', header=44, encoding='shift_jis', names=col_names) # csvをデータフレームとして読み込む\n",
    "#     list_.append(df)\n",
    "# frame = pd.concat(list_, join='inner') # joinをinnerに指定\n",
    "# frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Actual_generation_5min' + \"/Actual_generation2.csv\", encoding=\"shift_jis\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data in Tohoku area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read all the weather data in Tohoku area\n",
    "path ='/Users/kenotsu/Documents/Datasets/Master_thesis/Weather'\n",
    "frame = pd.DataFrame()\n",
    "# col_names = [\"Date\",\n",
    "#              \"Temp\", \"Temp_Qual\", \"Temp_Num\", \n",
    "#              \"SunLight(Time)\", \"SunLight(Time)_None\", \"SunLight(Time)_Qual\", \"SunLight(Time)_Num\", \n",
    "#              \"WindSpeed(m/s)\", \"WindSpeed(m/s)_Qual\", \"WindDirection\", \"WindDirection_Qual\", \"WindSpeed(m/s)_Num\", \n",
    "#              \"SunLight(MJ/㎡)\", \"SunLight(MJ/㎡)_Qual\", \"SunLight(MJ/㎡)_Num\", \n",
    "#              ]\n",
    "\n",
    "#Weather data in Aomori\n",
    "list_Aomori = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Ao\", \"Temp_Qual_Ao\", \"Temp_Num_Ao\", \n",
    "             \"SunLight(Time)_Ao\", \"SunLight(Time)_None_Ao\", \"SunLight(Time)_Qual_Ao\", \"SunLight(Time)_Num_Ao\", \n",
    "             \"WindSpeed(m/s)_Ao\", \"WindSpeed(m/s)_Qual_Ao\", \"WindDirection_Ao\", \"WindDirection_Qual_Ao\", \"WindSpeed(m/s)_Num_Ao\", \n",
    "             \"SunLight(MJ/㎡)_Ao\", \"SunLight(MJ/㎡)_Qual_Ao\", \"SunLight(MJ/㎡)_Num_Ao\", \n",
    "             ]\n",
    "allFiles_Aomori = sorted(glob.glob(path + \"/Aomori_*.csv\"))\n",
    "for file_Aomori in allFiles_Aomori:\n",
    "    df = pd.read_csv(file_Aomori, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Aomori.append(df)\n",
    "frame = pd.concat(list_Aomori, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Aomori.csv\", encoding=\"cp932\", index=False)\n",
    "\n",
    "#Weather data in Akita\n",
    "list_Akita = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Aki\", \"Temp_Qual_Aki\", \"Temp_Num_Aki\", \n",
    "             \"SunLight(Time)_Aki\", \"SunLight(Time)_None_Aki\", \"SunLight(Time)_Qual_Aki\", \"SunLight(Time)_Num_Aki\", \n",
    "             \"WindSpeed(m/s)_Aki\", \"WindSpeed(m/s)_Qual_Aki\", \"WindDirection_Aki\", \"WindDirection_Qual_Aki\", \"WindSpeed(m/s)_Num_Aki\", \n",
    "             \"SunLight(MJ/㎡)_Aki\", \"SunLight(MJ/㎡)_Qual_Aki\", \"SunLight(MJ/㎡)_Num_Aki\", \n",
    "             ]\n",
    "allFiles_Akita = sorted(glob.glob(path + \"/Akita_*.csv\"))\n",
    "for file_Akita in allFiles_Akita:\n",
    "    df = pd.read_csv(file_Akita, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Akita.append(df)\n",
    "frame = pd.concat(list_Akita, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Akita.csv\", encoding=\"cp932\", index=False)\n",
    "\n",
    "#Weather data in Morioka\n",
    "list_Morioka = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Mo\", \"Temp_Qual_Mo\", \"Temp_Num_Mo\", \n",
    "             \"SunLight(Time)_Mo\", \"SunLight(Time)_None_Mo\", \"SunLight(Time)_Qual_Mo\", \"SunLight(Time)_Num_Mo\", \n",
    "             \"WindSpeed(m/s)_Mo\", \"WindSpeed(m/s)_Qual_Mo\", \"WindDirection_Mo\", \"WindDirection_Qual_Mo\", \"WindSpeed(m/s)_Num_Mo\", \n",
    "             \"SunLight(MJ/㎡)_Mo\", \"SunLight(MJ/㎡)_Qual_Mo\", \"SunLight(MJ/㎡)_Num_Mo\", \n",
    "             ]\n",
    "allFiles_Morioka = sorted(glob.glob(path + \"/Morioka_*.csv\"))\n",
    "for file_Morioka in allFiles_Morioka:\n",
    "    df = pd.read_csv(file_Morioka, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Morioka.append(df)\n",
    "frame = pd.concat(list_Morioka, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Morioka.csv\", encoding=\"cp932\", index=False)\n",
    "\n",
    "#Weather data in Yamagata\n",
    "list_Yamagata = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Ya\", \"Temp_Qual_Ya\", \"Temp_Num_Ya\", \n",
    "             \"SunLight(Time)_Ya\", \"SunLight(Time)_None_Ya\", \"SunLight(Time)_Qual_Ya\", \"SunLight(Time)_Num_Ya\", \n",
    "             \"WindSpeed(m/s)_Ya\", \"WindSpeed(m/s)_Qual_Ya\", \"WindDirection_Ya\", \"WindDirection_Qual_Ya\", \"WindSpeed(m/s)_Num_Ya\", \n",
    "             \"SunLight(MJ/㎡)_Ya\", \"SunLight(MJ/㎡)_Qual_Ya\", \"SunLight(MJ/㎡)_Num_Ya\", \n",
    "             ]\n",
    "allFiles_Yamagata = sorted(glob.glob(path + \"/Yamagata_*.csv\"))\n",
    "for file_Yamagata in allFiles_Yamagata:\n",
    "    df = pd.read_csv(file_Yamagata, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Yamagata.append(df)\n",
    "frame = pd.concat(list_Yamagata, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Yamagata.csv\", encoding=\"cp932\", index=False)\n",
    "\n",
    "#Weather data in Sendai\n",
    "list_Sendai = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Se\", \"Temp_Qual_Se\", \"Temp_Num_Se\", \n",
    "             \"SunLight(Time)_Se\", \"SunLight(Time)_None_Se\", \"SunLight(Time)_Qual_Se\", \"SunLight(Time)_Num_Se\", \n",
    "             \"WindSpeed(m/s)_Se\", \"WindSpeed(m/s)_Qual_Se\", \"WindDirection_Se\", \"WindDirection_Qual_Se\", \"WindSpeed(m/s)_Num_Se\", \n",
    "             \"SunLight(MJ/㎡)_Se\", \"SunLight(MJ/㎡)_Qual_Se\", \"SunLight(MJ/㎡)_Num_Se\", \n",
    "             ]\n",
    "allFiles_Sendai = sorted(glob.glob(path + \"/Sendai_*.csv\"))\n",
    "for file_Sendai in allFiles_Sendai:\n",
    "    df = pd.read_csv(file_Sendai, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Sendai.append(df)\n",
    "frame = pd.concat(list_Sendai, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Sendai.csv\", encoding=\"cp932\", index=False)\n",
    "\n",
    "#Weather data in Fukushima\n",
    "list_Fukushima = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Fu\", \"Temp_Qual_Fu\", \"Temp_Num_Fu\", \n",
    "             \"SunLight(Time)_Fu\", \"SunLight(Time)_None_Fu\", \"SunLight(Time)_Qual_Fu\", \"SunLight(Time)_Num_Fu\", \n",
    "             \"WindSpeed(m/s)_Fu\", \"WindSpeed(m/s)_Qual_Fu\", \"WindDirection_Fu\", \"WindDirection_Qual_Fu\", \"WindSpeed(m/s)_Num_Fu\", \n",
    "             \"SunLight(MJ/㎡)_Fu\", \"SunLight(MJ/㎡)_Qual_Fu\", \"SunLight(MJ/㎡)_Num_Fu\", \n",
    "             ]\n",
    "allFiles_Fukushima = sorted(glob.glob(path + \"/Fukushima_*.csv\"))\n",
    "for file_Fukushima in allFiles_Fukushima:\n",
    "    df = pd.read_csv(file_Fukushima, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Fukushima.append(df)\n",
    "frame = pd.concat(list_Fukushima, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Fukushima.csv\", encoding=\"cp932\", index=False)\n",
    "\n",
    "#Weather data in Niigata\n",
    "list_Niigata = []\n",
    "col_names = [\"Date\",\n",
    "             \"Temp_Ni\", \"Temp_Qual_Ni\", \"Temp_Num_Ni\", \n",
    "             \"SunLight(Time)_Ni\", \"SunLight(Time)_None_Ni\", \"SunLight(Time)_Qual_Ni\", \"SunLight(Time)_Num_Ni\", \n",
    "             \"WindSpeed(m/s)_Ni\", \"WindSpeed(m/s)_Qual_Ni\", \"WindDirection_Ni\", \"WindDirection_Qual_Ni\", \"WindSpeed(m/s)_Num_Ni\", \n",
    "             \"SunLight(MJ/㎡)_Ni\", \"SunLight(MJ/㎡)_Qual_Ni\", \"SunLight(MJ/㎡)_Num_Ni\", \n",
    "             ]\n",
    "allFiles_Niigata = sorted(glob.glob(path + \"/Niigata_*.csv\"))\n",
    "for file_Niigata in allFiles_Niigata:\n",
    "    df = pd.read_csv(file_Niigata, sep=',', header=0, encoding='cp932', engine=\"python\", names=col_names, skiprows=[0,1,2,3,4,5]) # csvをデータフレームとして読み込む\n",
    "    list_Niigata.append(df)\n",
    "frame = pd.concat(list_Niigata, join='inner') # joinをinnerに指定\n",
    "frame.to_csv('/Users/kenotsu/Documents/Datasets/Master_thesis/Weather' + \"/Weather_Niigata.csv\", encoding=\"cp932\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記ループはもう少し短縮化できるはず(とくに列名の部分)\n",
    "\n",
    "for col in col_names:\n",
    "       if col  + \"_a\"\n",
    "       col\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "201px",
    "width": "439px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
